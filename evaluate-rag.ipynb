{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from elasticsearch import Elasticsearch\n",
    "from openai import OpenAI\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "\n",
    "random.seed(42)\n",
    "\n",
    "client = OpenAI()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents-with-ids.json', 'rt') as f_in:\n",
    "    documents = json.load(f_in)\n",
    "doc_idx = {d['doc_id']: d for d in documents}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'What is Garmin Connect and what purpose does it serve?',\n",
       " 'context': 'A web and mobile platform where users can track and analyze their fitness, activities, and wellness data',\n",
       " 'company_id': 'GRMN',\n",
       " 'doc_id': '46c6323c'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_idx['46c6323c']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ground_truth = pd.read_csv('ground-truth-data.csv')\n",
    "ground_truth = df_ground_truth.to_dict(orient='records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'multi-qa-MiniLM-L6-cos-v1'\n",
    "model = SentenceTransformer(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This file is created after Elasticsearch service and index are created using evaluate-retrieval.ipynb file. The file is operating while Elasticsearch is running and indexed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_client = Elasticsearch('http://localhost:9200') \n",
    "index_name = \"financial-faq\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elasticsearch Initilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def elastic_search_hybrid(field, query, vector, company_id):\n",
    "    knn_query = {\n",
    "        \"field\": field,\n",
    "        \"query_vector\": vector,\n",
    "        \"k\": 5,\n",
    "        \"num_candidates\": 10000,\n",
    "        \"boost\": 0.5,\n",
    "        \"filter\": {\n",
    "            \"term\": {\n",
    "                \"company_id\": company_id\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    keyword_query = {\n",
    "        \"bool\": {\n",
    "            \"must\": {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": query,\n",
    "                    \"fields\": [\"question\", \"context\"],\n",
    "                    \"type\": \"best_fields\",\n",
    "                    \"boost\": 0.5,\n",
    "                }\n",
    "            },\n",
    "            \"filter\": {\n",
    "                \"term\": {\n",
    "                    \"company_id\": company_id\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    search_query = {\n",
    "        \"knn\": knn_query,\n",
    "        \"query\": keyword_query,\n",
    "        \"size\": 5,\n",
    "        \"_source\": [\"question\", \"context\", \"company_id\", \"doc_id\"]\n",
    "    }\n",
    "\n",
    "    es_results = es_client.search(\n",
    "        index=index_name,\n",
    "        body=search_query\n",
    "    )\n",
    "    \n",
    "    result_docs = []\n",
    "    \n",
    "    for hit in es_results['hits']['hits']:\n",
    "        result_docs.append(hit['_source'])\n",
    "\n",
    "    return result_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_hybrid(search_function, q):\n",
    "    question = q['question']\n",
    "    company_id = q['company_id']\n",
    "\n",
    "    v_q = model.encode(question)\n",
    "\n",
    "    return elastic_search_hybrid(search_function, question, v_q, company_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM and RAG Initilisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    prompt_template = \"\"\"\n",
    "    You're a course financial assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "    Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "    QUESTION: {question}\n",
    "\n",
    "    CONTEXT: \n",
    "    {context}\n",
    "    \"\"\".strip()\n",
    "\n",
    "    context = \"\"\n",
    "    \n",
    "    for doc in search_results:\n",
    "        context = context + f\"question: {doc['question']}\\nanswer: {doc['context']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt, model='gpt-4o-mini'):\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query: dict, model='gpt-4o-mini') -> str:\n",
    "    search_results = search_hybrid('question_context_vector', query)\n",
    "    prompt = build_prompt(query['question'], search_results)\n",
    "    answer = llm(prompt, model=model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'What advanced features does Garmin’s SailAssist include?',\n",
       "  'company_id': 'GRMN',\n",
       "  'context': \"Garmin's SailAssist features include an enhanced wind rose with true and apparent wind data, POLAR tables, pre-race guidance, synchronized race timer, virtual starting line, time to burn, and lay line data fields.\",\n",
       "  'doc_id': '3b5858c1'},\n",
       " {'question': 'What are the characteristics of the Garmin quatix series wearable devices?',\n",
       "  'company_id': 'GRMN',\n",
       "  'context': 'The Garmin quatix series wearable devices are GPS-enabled smartwatches with features tailored for mariners, such as navigation, sailing features, stereo control, autopilot functions, tidal info, a built-in LED flashlight, and solar charging depending on the model.',\n",
       "  'doc_id': '36e4cff1'},\n",
       " {'question': 'What additional satellite systems do Garmin products utilize besides GPS?',\n",
       "  'company_id': 'GRMN',\n",
       "  'context': 'GLONASS, Galileo, BeiDou, and others',\n",
       "  'doc_id': 'bf63bcc5'},\n",
       " {'question': 'What unique features do Garmin smartwatches provide?',\n",
       "  'company_id': 'GRMN',\n",
       "  'context': 'Wrist-based heart rate monitoring, AMOLED displays, ECG app, Garmin Pay, music storage capabilities',\n",
       "  'doc_id': 'a15c4469'},\n",
       " {'question': 'What is Garmin’s strategy to be the provider of infotainment solutions in the automobile industry?',\n",
       "  'company_id': 'GRMN',\n",
       "  'context': 'Garmin is a tier-one supplier of infotainment solutions, offering a range of integrated multi-display platforms for premium audio and multimedia, navigation, cameras, and other automotive connectivity features, intended for premium vehicle manufacturers.',\n",
       "  'doc_id': 'e35e2e78'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_hybrid('question_context_vector', dict(question='What other advanced features does Garmin offer for sailing besides SailAssist?',\n",
    "         company_id = 'GRMN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Besides SailAssist, Garmin offers advanced features in their quatix series wearable devices, such as navigation, stereo control, autopilot functions, tidal info, and a built-in LED flashlight, along with solar charging depending on the model.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(dict(question='What other advanced features does Garmin offer for sailing besides SailAssist?',\n",
    "         company_id = 'GRMN'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool = ThreadPoolExecutor(max_workers=4)\n",
    "\n",
    "def map_progress(pool, seq, f):\n",
    "    results = []\n",
    "\n",
    "    with tqdm(total=len(seq)) as progress:\n",
    "        futures = []\n",
    "\n",
    "        for el in seq:\n",
    "            future = pool.submit(f, el)\n",
    "            future.add_done_callback(lambda p: progress.update())\n",
    "            futures.append(future)\n",
    "\n",
    "        for future in futures:\n",
    "            result = future.result()\n",
    "            results.append(result)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ChatGPT model daily limits fixed to 10000, therefore we cannot evaluate the entire dataset.\n",
    "# Therefore, I sampled the ground truth dataset with 10% which is around 2100.\n",
    "sample_size = 2100\n",
    "ground_truth_sample = random.sample(ground_truth, sample_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity and LLM as Judge for RAG Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity(record):\n",
    "    answer_orig = record['answer_orig']\n",
    "    answer_llm = record['answer_llm']\n",
    "    \n",
    "    v_llm = model.encode(answer_llm)\n",
    "    v_orig = model.encode(answer_orig)\n",
    "    \n",
    "    return v_llm.dot(v_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_as_judge_prompt = \"\"\"\n",
    "You are an expert evaluator for a Retrieval-Augmented Generation (RAG) system.\n",
    "Your task is to analyze the relevance of the generated answer compared to the original answer provided.\n",
    "Based on the relevance and similarity of the generated answer to the original answer, you will classify\n",
    "it as \"NON_RELEVANT\", \"PARTLY_RELEVANT\", or \"RELEVANT\".\n",
    "\n",
    "Here is the data for evaluation:\n",
    "\n",
    "Original Question: {question}\n",
    "Original Answer: {answer_orig}\n",
    "Generated Answer: {answer_llm}\n",
    "\n",
    "Please analyze the content and context of the generated answer in relation to the original\n",
    "answer and provide your evaluation in parsable JSON without using code blocks:\n",
    "\n",
    "{{\n",
    "  \"Relevance\": \"NON_RELEVANT\" | \"PARTLY_RELEVANT\" | \"RELEVANT\",\n",
    "  \"Explanation\": \"[Provide a brief explanation for your evaluation]\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_as_judge(samples):\n",
    "    json_evaluations = []\n",
    "\n",
    "    for record in tqdm(samples):\n",
    "        prompt = llm_as_judge_prompt.format(**record)\n",
    "        evaluation = llm(prompt, model='gpt-4o-mini')\n",
    "        json_eval = json.loads(evaluation)\n",
    "        json_evaluations.append(json_eval)\n",
    "    \n",
    "    df_evaluations = pd.DataFrame(json_evaluations)\n",
    "    \n",
    "    return df_evaluations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT-3.5 results and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(rec):\n",
    "    model = 'gpt-3.5-turbo'\n",
    "    answer_llm = rag(rec, model=model)\n",
    "    \n",
    "    doc_id = rec['document_id']\n",
    "    original_doc = doc_idx[doc_id]\n",
    "    answer_orig = original_doc['context']\n",
    "\n",
    "    return {\n",
    "        'answer_llm': answer_llm,\n",
    "        'answer_orig': answer_orig,\n",
    "        'document': doc_id,\n",
    "        'question': rec['question'],\n",
    "        'company_id': rec['company_id'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [07:40<00:00,  4.56it/s]\n"
     ]
    }
   ],
   "source": [
    "results_gpt35 = map_progress(pool, ground_truth_sample, process_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt35 = pd.DataFrame(results_gpt35)\n",
    "df_gpt35.to_csv('data/results-gpt35.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [00:32<00:00, 63.86it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2100.000000\n",
       "mean        0.636314\n",
       "std         0.270827\n",
       "min        -0.190102\n",
       "25%         0.410008\n",
       "50%         0.677585\n",
       "75%         0.865943\n",
       "max         1.000000\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = []\n",
    "\n",
    "for record in tqdm(results_gpt35):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity.append(sim)\n",
    "\n",
    "df_gpt35['cosine'] = similarity\n",
    "df_gpt35['cosine'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [06:20<00:00,  1.81s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Relevance\n",
       "RELEVANT           167\n",
       "NON_RELEVANT        22\n",
       "PARTLY_RELEVANT     21\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM AS JUDGE, resample it to 210\n",
    "df_gpt35_sample = df_gpt35.sample(n=210, random_state=1)\n",
    "gpt35_sample = df_gpt35_sample.to_dict(orient='records')\n",
    "\n",
    "gpt35_evaluations = llm_as_judge(gpt35_sample)\n",
    "\n",
    "gpt35_evaluations.Relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt35_evaluations.to_csv('data/results-gpt35-evaluations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatGPT-4o-mini results and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_record(rec):\n",
    "    model = 'gpt-4o-mini'\n",
    "    answer_llm = rag(rec, model=model)\n",
    "    \n",
    "    doc_id = rec['document_id']\n",
    "    original_doc = doc_idx[doc_id]\n",
    "    answer_orig = original_doc['context']\n",
    "\n",
    "    return {\n",
    "        'answer_llm': answer_llm,\n",
    "        'answer_orig': answer_orig,\n",
    "        'document': doc_id,\n",
    "        'question': rec['question'],\n",
    "        'company_id': rec['company_id'],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [12:38<00:00,  2.77it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [12:38<00:00,  2.27it/s]"
     ]
    }
   ],
   "source": [
    "results_gpt4o = map_progress(pool, ground_truth_sample, process_record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt4o = pd.DataFrame(results_gpt4o)\n",
    "df_gpt4o.to_csv('data/results-gpt4o.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2100/2100 [00:32<00:00, 64.64it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    2100.000000\n",
       "mean        0.630666\n",
       "std         0.252160\n",
       "min        -0.074854\n",
       "25%         0.432983\n",
       "50%         0.668283\n",
       "75%         0.831950\n",
       "max         1.000000\n",
       "Name: cosine, dtype: float64"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = []\n",
    "\n",
    "for record in tqdm(results_gpt4o):\n",
    "    sim = compute_similarity(record)\n",
    "    similarity.append(sim)\n",
    "\n",
    "df_gpt4o['cosine'] = similarity\n",
    "df_gpt4o['cosine'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 210/210 [06:12<00:00,  1.77s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Relevance\n",
       "RELEVANT           172\n",
       "PARTLY_RELEVANT     26\n",
       "NON_RELEVANT        12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM AS JUDGE, resample it to 210\n",
    "df_gpt4o_sample = df_gpt4o.sample(n=210, random_state=1)\n",
    "gpt4o_sample = df_gpt4o_sample.to_dict(orient='records')\n",
    "\n",
    "gpt4o_evaluations = llm_as_judge(gpt4o_sample)\n",
    "\n",
    "gpt4o_evaluations.Relevance.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt4o_evaluations.to_csv('data/results-gpt4o-evaluations.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````\n",
    "count    2100.000000\n",
    "mean        0.630666\n",
    "std         0.252160\n",
    "min        -0.074854\n",
    "25%         0.432983\n",
    "50%         0.668283\n",
    "75%         0.831950\n",
    "max         1.000000\n",
    "Name: cosine, dtype: float64\n",
    "\n",
    "Relevance\n",
    "RELEVANT           172\n",
    "PARTLY_RELEVANT     26\n",
    "NON_RELEVANT        12\n",
    "Name: count, dtype: int64\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPT-4o-mini answer prompts better with retreived context rather than GPT-3.5. For app and interface hybrid search (question-context-vector) and GPT-4o-mini will operate."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
